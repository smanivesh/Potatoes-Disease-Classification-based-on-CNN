# -*- coding: utf-8 -*-
"""Potato Disease Classification based on CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qpfGIwvypZzZoaZSPo8UCOS51urGToMG

## Data loading and Data Cleaning
"""

import tensorflow as tf
from tensorflow.keras import models, layers
import matplotlib.pyplot as plt

BATCH_SIZE=32
IMAGE_SIZE= 256
CHANNELS=3
ePOCHS=50

directory_path = "/content/drive/MyDrive/PlantVillage"

dataset= tf.keras.preprocessing.image_dataset_from_directory(
    directory_path,
    shuffle=True,
    image_size=(IMAGE_SIZE, IMAGE_SIZE),
    batch_size=BATCH_SIZE
)

class_names= dataset.class_names
class_names

for image_batch, labels_batch in dataset.take(1):
  print(image_batch.shape)
  print(labels_batch.numpy())
  for i in range(12):
    ax= plt.subplot(3,4,i+1)
    plt.imshow(image_batch[i].numpy().astype("uint8"))
    plt.title(class_names[labels_batch[i]])
    plt.axis("off")

"""## Dividing dataset for training and testing

80% => Training 10% => validation 10% > test
"""

train_size= 0.8
len(dataset)* train_size

train_ds= dataset.take(54)
len(train_ds)

test_ds= dataset.skip(54)
len(test_ds)

val_size=0.1
len(dataset)*val_size

test_ds= dataset.skip(54)
len(test_ds)

val_size=0.1
len(dataset)* val_size

test_ds= test_ds.take(6)
len(test_ds)

def get_dataset_partitions_tf(ds, train_split=0.8, val_split=0.1, test_split=0.1, shuffle= True, shuffle_size= 10000):
  assert(train_split+ test_split+ val_split)==1

  ds_size= len(ds)

  if shuffle:
    ds= ds.shuffle(shuffle_size, seed=12)

  train_size= int(train_split*ds_size) 
  val_size= int(val_split* ds_size) 

  train_ds= ds.take(train_size)
  val_ds= ds.skip(train_size).take(val_size)
  test_ds= ds.skip(train_size).take(val_size)

  return train_ds, val_ds, test_ds

train_ds, val_ds, test_ds= get_dataset_partitions_tf(dataset)

len(train_ds)

len(val_ds)

len(test_ds)

"""## Catching, shuffle & prefetch dataset"""

train_ds=train_ds.cache().shuffle(1000).prefetch(buffer_size= tf.data.AUTOTUNE)
val_ds=val_ds.cache().shuffle(1000).prefetch(buffer_size= tf.data.AUTOTUNE)
test_ds=test_ds.cache().shuffle(1000).prefetch(buffer_size= tf.data.AUTOTUNE)

# normalizing
for image_batch, labels_batch in dataset.take(1):
  print(image_batch[0].numpy()/255)

resize_and_rescale = tf.keras.Sequential([
    # image resize 255x256
    layers.experimental.preprocessing.Resizing(IMAGE_SIZE, IMAGE_SIZE),
    layers.experimental.preprocessing.Rescaling(1.0/255)
]
)

"""## Data augmentation"""

data_augmentation = tf.keras.Sequential([
    # image resize 255x256
    layers.experimental.preprocessing.RandomFlip("horizontal_and_vertical"),
    layers.experimental.preprocessing.RandomRotation(0.2),
]
)

"""## Creating Models"""

# CNN
input_shape= (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)

n_classes=3

model= models.Sequential([
    resize_and_rescale,
    data_augmentation,
    layers.Conv2D(32, kernel_size=(3,3), activation='relu', input_shape= input_shape),
    layers.MaxPooling2D((2,2)),
    layers.Conv2D(64, kernel_size=(3,3), activation='relu'),
    layers.MaxPooling2D((2,2)),
    layers.Conv2D(64, kernel_size=(3,3), activation='relu'),
    layers.MaxPooling2D((2,2)),
    layers.Conv2D(64, kernel_size=(3,3), activation='relu'),
    layers.MaxPooling2D((2,2)),
    layers.Conv2D(64, kernel_size=(3,3), activation='relu'),
    layers.MaxPooling2D((2,2)),
    layers.Conv2D(64, kernel_size=(3,3), activation='relu'),
    layers.MaxPooling2D((2,2)),

    # FLatten
    layers.Flatten(),
    layers.Dense(64, activation='relu'),

    layers.Dense(n_classes, activation='softmax'),

])

model.build(input_shape= input_shape)

model.summary()

"""## Compiling Model"""

model.compile(
    optimizer= 'adam',
    loss= tf.keras.losses.SparseCategoricalCrossentropy(from_logits= False),
    metrics=['accuracy']
)

history= model.fit(
    train_ds,
    batch_size= BATCH_SIZE,
    validation_data= val_ds,
    verbose=1,
    epochs= ePOCHS
)

scores= model.evaluate(test_ds)

scores

history.params

history.history.keys()

history.history['loss']

len(history.history['loss'])

acc= history.history['accuracy']
val_acc= history.history['val_accuracy']

loss= history.history['loss']
val_loss= history.history['val_loss']

plt.figure(figsize(8,8))
plt.subplot(1,2,1)
plt.plot(range(ePOCHS), acc,label='Training Accuracy')
plt.plot(range(ePOCHS), val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1,2,2)
plt.plot(range(ePOCHS), loss, label='Training Loss')
plt.plot(range(ePOCHS), val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

import numpy as np
for images_batch, labels_batch in test_ds.take(6):
  first_image= images_batch[0].numpy().astype("uint8")

  print("First image to predict: ")
  plt.imshow(first_image)
  print("Actual Label:", class_names[labels_batch[0].numpy()])

  batch_prediction= model.predict(images_batch)
  print("Predicted Label:",class_names[np.argmax(batch_prediction[0])])  # max number ka index

# from PIL import Image
# import numpy as np

# pot_healthy= '/content/drive/MyDrive/PlantVillage/Potato___healthy'

# import os

# folder_path = '/content/drive/MyDrive/PlantVillage/Potato___Early_blight'

# # Iterate over the files in the folder
# for filename in os.listdir(folder_path):
#     # Check if the file has an image extension (e.g., .jpg, .png)
#     if filename.endswith('.jpg') or filename.endswith('.png')or filename.endswith('.JPG'):
#         # Print the absolute path of the image
#         image_path = os.path.join(folder_path, filename)
#         image = Image.open(image_path)
#         image = image.resize((256, 256))  # Adjust size as required
#         image = np.array(image) / 255.0  # Normalize pixel values
#         # image= image.numpy().astype("uint8")

#         # Add an extra dimension to match the input shape expected by the model
#         image = np.expand_dims(image, axis=0)

#         # Make the prediction
#         predictions = model.predict(image)

#         # Get the predicted class
#         # predicted_class = np.argmax(predictions)
#         prediction_class= class_names[np.argmax(predictions[0])]

#         # Define a mapping for class labels
#         # class_labels = ['Early Blight', 'Late Blight', 'Healthy']

#         # Print the predicted class
#         print('Predicted class:', prediction_class)
#         # print('Predicted class:', predicted_class)

# image_path='/content/drive/MyDrive/PlantVillage/Potato___Early_blight/00d8f10f-5038-4e0f-bb58-0b885ddc0cc5___RS_Early.B 8722.JPG'
# image = Image.open(image_path)
# image = image.resize((256, 256) ) # Adjust size as required
# # image = np.array(image) / 255.0  # Normalize pixel values
# image= image.numpy().astype("uint8")

# # Add an extra dimension to match the input shape expected by the model
# # image = np.expand_dims(image, axis=0)

# # Make the prediction
# predictions = model.predict(image)
# print(predictions)
# # Get the predicted class
# # predicted_class = np.argmax(predictions)
# print(np.argmax(predictions[0]))
# prediction_class= class_names[np.argmax(predictions[0])]

# # Define a mapping for class labels
# # class_labels = ['Early Blight', 'Late Blight', 'Healthy']

# # Print the predicted class
# print('Predicted class:', prediction_class)
# # print('Predicted class:', predicted_class)